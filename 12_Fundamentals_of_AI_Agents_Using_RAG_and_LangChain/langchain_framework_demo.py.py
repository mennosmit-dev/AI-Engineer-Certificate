"""
Objectives
- Grasp the core features of Langchain, including prompt templates, chains, and agents, emphasizing its role in enhancing LLM customization and output relevance. **(Framework understanding)**:
- Explore LangChain's modular flexibility, which allows for dynamic adjustments to prompts and models without extensive code changes. **(Modular approach)**
- Discover how to enhance LLM applications by integrating retrieval-augmented generation (RAG) techniques with LangChain. This enables more accurate and context-aware responses by leveraging external data sources. **(Retrieval-augmented integration)**

Installing required libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install --force-reinstall --no-cache-dir tenacity --user
# !pip install "ibm-watsonx-ai==1.0.4" --user
# !pip install "ibm-watson-machine-learning==1.0.357" --user
# !pip install "langchain-ibm==0.1.7" --user
# !pip install "langchain-community==0.2.1" --user
# !pip install "langchain-experimental==0.0.59" --user
# !pip install "langchainhub==0.1.17" --user
# !pip install "langchain==0.2.1" --user
# !pip install "pypdf==4.2.0" --user
# !pip install "chromadb == 0.4.24" --user

"""
Importing required libraries
"""

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes
from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM

"""
LangChain concepts
"""

model_id = 'mistralai/mixtral-8x7b-instruct-v01'

parameters = {
    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output
    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses
}

credentials = {
    "url": "https://us-south.ml.cloud.ibm.com"
}

project_id = "skills-network"

model = ModelInference(
    model_id=model_id,
    params=parameters,
    credentials=credentials,
    project_id=project_id
)

"""
Let's use a simple example to let the model generate some text:
"""

msg = model.generate("In today's sales meeting, we ")
print(msg['results'][0]['generated_text'])

"""
Chat model
"""

mixtral_llm = WatsonxLLM(model = model)

"""
The following provides an example of an interaction with a `WatsonLLM()`-wrapped model:
"""

print(mixtral_llm.invoke("Who is man's best friend?"))

"""
Chat message
"""

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

"""
Now let's create a few messages that simulate a chat experience with the bot:
"""

msg = mixtral_llm.invoke(
    [
        SystemMessage(content="You are a helpful AI bot that assists a user in choosing the perfect book to read in one short sentence"),
        HumanMessage(content="I enjoy mystery novels, what should I read?")
    ]
)

print(msg)

"""
Notice that the model responded with an `AI` message.
"""

msg = mixtral_llm.invoke(
    [
        SystemMessage(content="You are a supportive AI bot that suggests fitness activities to a user in one short sentence"),
        HumanMessage(content="I like high-intensity workouts, what should I do?"),
        AIMessage(content="You should try a CrossFit class"),
        HumanMessage(content="How often should I attend?")
    ]
)

print(msg)

"""
You can also exclude the system message if you want:
"""

msg = mixtral_llm.invoke(
    [
        HumanMessage(content="What month follows June?")
    ]
)

print(msg)

"""
Prompt templates
String prompt templates
"""

from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("Tell me one {adjective} joke about {topic}")
input_ = {"adjective": "funny", "topic": "cats"}  # create a dictionary to store the corresponding input to placeholders in prompt template

prompt.invoke(input_)

"""
Chat prompt templates
"""

from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me a joke about {topic}")
])

input_ = {"topic": "cats"}

prompt.invoke(input_)

"""
Messages place holder
"""

from langchain_core.prompts import MessagesPlaceholder
from langchain_core.messages import HumanMessage

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder("msgs")
])

input_ = {"msgs": [HumanMessage(content="What is the day after Tuesday?")]}

prompt.invoke(input_)

"""
You could wrap the prompt and the chat model and pass them into a chain, which could invoke the message.
"""

chain = prompt | mixtral_llm
response = chain.invoke(input = input_)
print(response)

"""
Example selectors
"""

from langchain_core.example_selectors import LengthBasedExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)
example_selector = LengthBasedExampleSelector(
    examples=examples,
    example_prompt=example_prompt,
    max_length=25,  # The maximum length that the formatted examples should be.
)
dynamic_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)

"""
An example with small input, so it selects all examples.
"""

print(dynamic_prompt.format(adjective="big"))

"""
An example with long input, so it selects only one example.
"""

long_string = "big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else"
print(dynamic_prompt.format(adjective=long_string))

"""
Output parsers
"""

from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field

# Define your desired data structure.
class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")

# And a query intented to prompt a language model to populate the data structure.
joke_query = "Tell me a joke."

# Set up a parser + inject instructions into the prompt template.
output_parser = JsonOutputParser(pydantic_object=Joke)

format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": format_instructions},
)

chain = prompt | mixtral_llm | output_parser

chain.invoke({"query": joke_query})

"""
Comma separated list parser
"""

from langchain.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()

format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template="Answer the user query. {format_instructions}\nList five {subject}.",
    input_variables=["subject"],
    partial_variables={"format_instructions": format_instructions},
)

chain = prompt | mixtral_llm | output_parser

chain.invoke({"subject": "ice cream flavors"})

"""
Documents
"""

from langchain_core.documents import Document

Document(page_content="""Python is an interpreted high-level general-purpose programming language.
                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.""",
         metadata={
             'my_document_id' : 234234,
             'my_document_source' : "About Python",
             'my_document_create_time' : 1680013019
         })

"""
Note that you don't have to include metadata if you don't want to:
"""

Document(page_content="""Python is an interpreted high-level general-purpose programming language.
                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.""")

"""
Document loaders

PDF loader
"""

from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf")

document = loader.load()

"""
Here, `document` is a `Document` object with `page_content` and `metadata`:
"""

document[2]  # take a look at the page 2

print(document[1].page_content[:1000])  # print the page 1's first 1000 tokens

"""
URL and website loader
"""

from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://python.langchain.com/v0.2/docs/introduction/")

web_data = loader.load()

print(web_data[0].page_content[:1000])

"""
Text splitters
"""

from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator="\n")  # define chunk_size which is length of characters, and also separator.
chunks = text_splitter.split_documents(document)
print(len(chunks))

"""
It splits the document into 148 chunks. Let's look at the content of a chunk:
"""

chunks[5].page_content   # take a look at any chunk's page content

"""
Embedding models
"""

from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames

embed_params = {
    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,
    EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
}

from langchain_ibm import WatsonxEmbeddings

watsonx_embedding = WatsonxEmbeddings(
    model_id="ibm/slate-125m-english-rtrvr",
    url="https://us-south.ml.cloud.ibm.com",
    project_id="skills-network",
    params=embed_params,
)

"""
The following embeds content in each of the chunks. You can then output the first 5 numbers in the vector representation of the content of the first chunk:
"""

texts = [text.page_content for text in chunks]

embedding_result = watsonx_embedding.embed_documents(texts)
embedding_result[0][:5]

"""
Vector stores
"""

from langchain.vectorstores import Chroma

"""
You have the embedding model perform the embedding process and store the resulting vectors in the Chroma vector database.
"""

docsearch = Chroma.from_documents(chunks, watsonx_embedding)

"""
Then, you could use a similarity search strategy to retrieve the information that is related to the query you set.
"""

query = "Langchain"
docs = docsearch.similarity_search(query)
print(docs[0].page_content)

"""
Retrievers

Vector store-backed retriever
"""

retriever = docsearch.as_retriever()

docs = retriever.invoke("Langchain")

docs[0]

"""
Note that the results are identical to the ones obtained using the similarity search strategy.

Parent document retriever
"""

from langchain.retrievers import ParentDocumentRetriever
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.storage import InMemoryStore

# Set two splitters. One is with big chunk size (parent) and one is with small chunk size (child)
parent_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=20, separator='\n')
child_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=20, separator='\n')

vectorstore = Chroma(
    collection_name="split_parents", embedding_function=watsonx_embedding
)

# The storage layer for the parent documents
store = InMemoryStore()

retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter,
)

retriever.add_documents(document)

"""
These are a number of large chunks.
"""

len(list(store.yield_keys()))

"""
Let's make sure the underlying vector store still retrieves the small chunks.
"""

sub_docs = vectorstore.similarity_search("Langchain")

print(sub_docs[0].page_content)

"""
And then retrieve the relevant large chunk.
"""

retrieved_docs = retriever.invoke("Langchain")

print(retrieved_docs[0].page_content)

"""
RetrievalQA
"""

from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(llm=mixtral_llm,
                                 chain_type="stuff",
                                 retriever=docsearch.as_retriever(),
                                 return_source_documents=False)
query = "what is this paper discussing?"
qa.invoke(query)

"""
Memory

Chat message history
"""

from langchain.memory import ChatMessageHistory

chat = mixtral_llm

history = ChatMessageHistory()

history.add_ai_message("hi!")

history.add_user_message("what is the capital of France?")

"""
Let's have a look at the messages in the history:
"""

history.messages

"""
You can pass these messages in history to the model to generate a response:
"""

ai_response = chat.invoke(history.messages)
ai_response

"""
You can see the model gives a proper response.
"""

history.add_ai_message(ai_response)
history.messages

"""
Conversation buffer
"""

from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

conversation = ConversationChain(
    llm=mixtral_llm,
    verbose=True,
    memory=ConversationBufferMemory()
)

"""
Let’s begin the conversation by introducing the user as a little cat and proceed by incorporating some additional messages. Finally, prompt the model to check if it can recall that the user is a little cat.
"""

conversation.invoke(input="Hello, I am a little cat. Who are you?")

conversation.invoke(input="What can you do?")

conversation.invoke(input="Who am I?.")

"""
Chains

Simple LLMChain
"""

from langchain.chains import LLMChain

template = """Your job is to come up with a classic dish from the area that the users suggests.
                {location}

                YOUR RESPONSE:
"""
prompt_template = PromptTemplate(template=template, input_variables=['location'])

# chain 1
location_chain = LLMChain(llm=mixtral_llm, prompt=prompt_template, output_key='meal')

location_chain.invoke(input={'location':'China'})

"""
Simple sequential chain
"""

from langchain.chains import SequentialChain

template = """Given a meal {meal}, give a short and simple recipe on how to make that dish at home.

                YOUR RESPONSE:
"""
prompt_template = PromptTemplate(template=template, input_variables=['meal'])

# chain 2
dish_chain = LLMChain(llm=mixtral_llm, prompt=prompt_template, output_key='recipe')

template = """Given the recipe {recipe}, estimate how much time I need to cook it.

                YOUR RESPONSE:
"""
prompt_template = PromptTemplate(template=template, input_variables=['recipe'])

# chain 3
recipe_chain = LLMChain(llm=mixtral_llm, prompt=prompt_template, output_key='time')

# overall chain
overall_chain = SequentialChain(chains=[location_chain, dish_chain, recipe_chain],
                                      input_variables=['location'],
                                      output_variables=['meal', 'recipe', 'time'],
                                      verbose= True)

from pprint import pprint

"""Let's use ```pprint``` to print the response to make it more clear.

"""

pprint(overall_chain.invoke(input={'location':'China'}))

"""
Summarization chain
"""

from langchain.chains.summarize import load_summarize_chain

chain = load_summarize_chain(llm=mixtral_llm, chain_type="stuff", verbose=False)
response = chain.invoke(web_data)

print(response['output_text'])

"""
Agents

Tools
"""

from langchain.agents import Tool
from langchain_experimental.utilities import PythonREPL

python_repl = PythonREPL()

"""
Let's pass a simple Python command here as the input to let the tool excute.
"""

python_repl.run("a = 3; b = 1; print(a+b)")

"""
Toolkits
"""

from langchain_experimental.tools import PythonREPLTool

tools = [PythonREPLTool()]

"""
Agents
"""

from langchain.agents import create_react_agent
from langchain import hub
from langchain.agents import AgentExecutor

instructions = """You are an agent designed to write and execute python code to answer questions.
You have access to a python REPL, which you can use to execute python code.
If you get an error, debug your code and try again.
Only use the output of your code to answer the question.
You might know the answer without running any code, but you should still run the code to get the answer.
If it does not seem like you can write code to answer the question, just return "I don't know" as the answer.
"""

# here you will use the prompt directly from the langchain hub
base_prompt = hub.pull("langchain-ai/react-agent-template")
prompt = base_prompt.partial(instructions=instructions)

"""
You'll use the `create_react_agent` agent. It combines reasoning (e.g., Chain-of-Thought (CoT) prompting) and acting (e.g., action plan generation) together to let the LLM solve questions like humans would.
"""

agent = create_react_agent(mixtral_llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)  # tools were defined in the toolkit part above

"""
Let's ask a coding question to solve LLM problem:
"""

agent_executor.invoke(input = {"input": "What is the 3rd fibonacci number?"})

"""# Exercises

### Exercise 1: Try with another LLM

Watsonx.ai provides access to several foundational models. In this lab, used `mistralai/mixtral-8x7b-instruct-v01` has been used. Try using another foundational model, such as `'meta-llama/llama-3-3-70b-instruct'`.
"""

model_id = 'meta-llama/llama-3-3-70b-instruct'

parameters = {
    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output
    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses
}

credentials = {
    "url": "https://us-south.ml.cloud.ibm.com"
}

project_id = "skills-network"

model = ModelInference(
    model_id=model_id,
    params=parameters,
    credentials=credentials,
    project_id=project_id
)

### Exercise 2: Split the document with another separator

Can you use another separator to split the document and see how types of chunks are created? For example, use "." as a separator.
"""
    
text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator=".")  # define chunk_size which is length of characteres, and also separator.
chunks = text_splitter.split_documents(document)
print(len(chunks))

print(chunks[5].page_content)

### Exercise 3: Create an agent to talk with CSV data

Imagine you have a CSV file that you would like an LLM to read and analyze for you. This way, you only need to ask the LLM, and it can return the answer to you. You can refer to [https://python.langchain.com/v0.2/docs/integrations/toolkits/csv/](https://python.langchain.com/v0.2/docs/integrations/toolkits/csv/) for more details on the agent you should use. You can use this URL (https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZNoKMJ9rssJn-QbJ49kOzA/student-mat.csv) to load a sample CSV file.
"""

from langchain.agents.agent_types import AgentType
from langchain_experimental.agents.agent_toolkits import create_csv_agent
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
import pandas as pd

df = pd.read_csv(
    "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZNoKMJ9rssJn-QbJ49kOzA/student-mat.csv"
)

agent = create_pandas_dataframe_agent(
    mixtral_llm,
    df,
    verbose=True,
    return_intermediate_steps=True
)

response = agent.invoke("How many rows in the dataframe?",)

print(response['output'])


## Other contributors

[Wojciech Fulmyk](https://author.skills.network/instructors/wojciech_fulmyk)

Wojciech "Victor" Fulmyk is a Data Scientist at IBM. He is also a PhD Candidate in Economics in the University of Calgary.

[Faranak Heidari](https://author.skills.network/instructors/faranak_heidari) is a Data Scientist at IBM and PhD candidate at the University of Toronto.

© Copyright IBM Corporation. All rights reserved.
"""
