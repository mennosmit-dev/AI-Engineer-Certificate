"""
Loaded and utilised several hugging face models to manually or via the pipeline function perform sentiment classification (DistilBERT) and text generation (GPT-2).

Objectives
- Learn to set up and use the Hugging Face `transformers` library.
- Perform text classification and text generation using DistilBERT and GPT-2 models without `pipeline()`.
- Understand and utilize the `pipeline()` function to simplify various NLP tasks.
- Compare the ease of using models directly versus using the `pipeline()` function.

Installing required libraries
"""
!pip install torch
!pip install transformers

from transformers import pipeline
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

"""
Load the model and tokenizer
"""
# Load the tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

"""
Preprocess the input text
"""
# Sample text
text = "Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim."

# Tokenize the input text
inputs = tokenizer(text, return_tensors="pt")

print(inputs)

"""
Perform inference
"""
# Perform inference
with torch.no_grad():
    outputs = model(**inputs)

"""
Another method is `input_ids`, and `attention_mask` is their own parameter.
"""
#model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])

"""
Get the logits
"""
logits = outputs.logits
logits.shape

"""
Post-process the output
"""
# Convert logits to probabilities
probs = torch.softmax(logits, dim=-1)

# Get the predicted class
predicted_class = torch.argmax(probs, dim=-1)

# Map the predicted class to the label
labels = ["NEGATIVE", "POSITIVE"]
predicted_label = labels[predicted_class]

print(f"Predicted label: {predicted_label}")

"""
Text generation with GPT-2
"""
# Load the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

"""
Load the pretrained GPT-2 model with a language modeling head. The model generates text based on the input tokens.
"""
# Load the tokenizer and model

model = GPT2LMHeadModel.from_pretrained("gpt2")

"""
Preprocess the input text  
"""
# Prompt
prompt = "Once upon a time"

# Tokenize the input text
inputs = tokenizer(prompt, return_tensors="pt")
inputs

"""
Perform inference  
"""
# Generate text
output_ids = model.generate(
    inputs.input_ids,
    attention_mask=inputs.attention_mask,
    pad_token_id=tokenizer.eos_token_id,
    max_length=50,
    num_return_sequences=1
)

output_ids

"""
Post-process the output  
"""
# Decode the generated text
generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print(generated_text)

"""
Hugging Face `pipeline()` function

### Example 1: Text classification using `pipeline()`

In this example, you will use the `pipeline()` function to perform text classification. You will load a pretrained text classification model and use it to classify a sample text.

#### Load the text classification model:
We initialize the pipeline for the `text-classification` task, specifying the model `"distilbert-base-uncased-finetuned-sst-2-english"`. This model is fine-tuned for sentiment analysis.

#### Classify the sample text:
We use the classifier to classify a sample text: "Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim." The `classifier` function returns the classification result, which is then printed.
"""
# Load a general text classification model
classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")

# Classify a sample text
result = classifier("Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.")
print(result)

"""
Output

### Example 2: Language detection using `pipeline()`
"""
from transformers import pipeline

classifier = pipeline("text-classification", model="papluca/xlm-roberta-base-language-detection")
result = classifier("Bonjour, comment ça va?")
print(result)

"""
Output

### Example 3: Text generation using `pipeline()`
"""
# Initialize the text generation pipeline with GPT-2
generator = pipeline("text-generation", model="gpt2")

"""
Generate text based on a given prompt:
"""
# Generate text based on a given prompt
prompt = "Once upon a time"
result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)

# Print the generated text
print(result[0]['generated_text'])

"""
Output

### Example 4: Text generation using T5 with `pipeline()`
"""
# Initialize the text generation pipeline with T5
generator = pipeline("text2text-generation", model="t5-small")

"""
Generate text based on a given prompt:
"""
# Generate text based on a given prompt
prompt = "translate English to French: How are you?"
result = generator(prompt, max_length=50, num_return_sequences=1)

# Print the generated text
print(result[0]['generated_text'])

"""Output

## Benefits of using `pipeline()`

- **Reduced Boilerplate Code**: Simplifies the code required to perform NLP tasks.
- **Improved Readability**: Makes code more readable and expressive.
- **Time Efficiency**: Saves time by handling model loading, tokenization, inference, and post-processing automatically.
- **Consistent API**: Provides a consistent API across different tasks, allowing for easy experimentation and quick prototyping.
- **Automatic Framework Handling**: Automatically handles the underlying framework (TensorFlow or PyTorch).

## When to use `pipeline()`

- **Quick Prototyping**: When you need to quickly prototype an NLP application or experiment with different models.
- **Simple Tasks**: When performing simple or common NLP tasks that are well-supported by the `pipeline()` function.
- **Deployment**: When deploying NLP models in environments where simplicity and ease of use are crucial.

## When to avoid `pipeline()`

- **Custom Tasks**: When you need to perform highly customized tasks that are not well-supported by the `pipeline()` function.
- **Performance Optimization**: When you need fine-grained control over the model and tokenization process for performance optimization or specific use cases.

# Exercise: Fill-mask task using BERT with `pipeline()`

In this exercise, you will use the `pipeline()` function to perform a fill-mask task using the BERT model. You will load a pretrained BERT model and use it to predict the masked word in a given sentence.

### Instructions

1. **Initialize the fill-mask pipeline** with the BERT model.
2. **Create a prompt** with a masked token.
3. **Generate text** by filling in the masked token.
4. **Print the generated text** with the predictions.
"""

prompt = 'This is something that [MASK] very interesting'

fill_mask = pipeline("fill-mask", model="bert-base-uncased")
result = fill_mask(prompt)

print(result)

"""
```

© Copyright IBM Corporation. All rights reserved.
"""
