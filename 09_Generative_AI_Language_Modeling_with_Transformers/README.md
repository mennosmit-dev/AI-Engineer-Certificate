# Course 9: Generative AI Language Modeling with Transformers

This folder contains coursework and projects completed for the **[Generative AI Language Modeling with Transformers](https://www.coursera.org/learn/generative-ai-language-modeling-with-transformers?specialization=ai-engineer)** course, part of the [IBM AI Engineer Professional Certificate](https://www.coursera.org/professional-certificates/ai-engineer) on Coursera.

## ðŸ§  Course Description

This course provides an in-depth overview of transformer-based models for natural language processing (NLP). Learners explore both encoder and decoder architectures, focusing on their applications in tasks like text classification and language translation.

By the end of this course, you will be able to:

- Explain attention mechanisms in transformers and their role in capturing contextual information.
- Describe language modeling using decoder-based models like GPT and encoder-based models like BERT.
- Implement positional encoding, masking, and attention mechanisms in PyTorch.
- Utilize transformer-based models for text classification and language translation tasks.

---

## ðŸ“‚ Contents: The coding projects I worked on

- `attention_pos_encoding_models.py`: Experimented with attention mechanism for translation models and more general setups involing multi-head attention, with several postitional encoding layers, and with Transformer Encoder and Decoder models, building a broad intuÃ¯tion around it.

---

## ðŸ”§ Tools and Libraries

- Python
- Jupyter Notebooks
- PyTorch
- Hugging Face Transformers
- NumPy
- Matplotlib

---

## ðŸ“Œ Certificate Series

This is the ninth course in the [IBM AI Engineer Professional Certificate](https://www.coursera.org/professional-certificates/ai-engineer).
