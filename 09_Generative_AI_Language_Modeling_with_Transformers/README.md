# Course 9: Generative AI Language Modeling with Transformers

This folder contains coursework and projects completed for the **[Generative AI Language Modeling with Transformers](https://www.coursera.org/learn/generative-ai-language-modeling-with-transformers?specialization=ai-engineer)** course, part of the [IBM AI Engineer Professional Certificate](https://www.coursera.org/professional-certificates/ai-engineer) on Coursera.

## ðŸ§  Course Description

This course provides an in-depth overview of transformer-based models for natural language processing (NLP). Learners explore both encoder and decoder architectures, focusing on their applications in tasks like text classification and language translation.

By the end of this course, you will be able to:

- Explain attention mechanisms in transformers and their role in capturing contextual information.
- Describe language modeling using decoder-based models like GPT and encoder-based models like BERT.
- Implement positional encoding, masking, and attention mechanisms in PyTorch.
- Utilize transformer-based models for text classification and language translation tasks.

---

## ðŸ“‚ Contents: The coding projects I worked on

- `transformer_language_modeling.py`: Placeholder project demonstrating the implementation of transformer architectures for NLP tasks. *(More projects coming soon...)*

---

## ðŸ”§ Tools and Libraries

- Python
- Jupyter Notebooks
- PyTorch
- Hugging Face Transformers
- NumPy
- Matplotlib

---

## ðŸ“Œ Certificate Series

This is the ninth course in the [IBM AI Engineer Professional Certificate](https://www.coursera.org/professional-certificates/ai-engineer).
