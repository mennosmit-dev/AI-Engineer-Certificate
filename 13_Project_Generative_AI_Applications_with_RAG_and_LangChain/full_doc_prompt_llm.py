"""
Objectives
 - Explain the concept of context length for LLMs.
 - Recognize the limitations of retrieving information when inputting the entire content of a document into a prompt.

Installing required libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# #After executing the cell,please RESTART the kernel and run all the cells.
# !pip install --user "ibm-watsonx-ai==1.0.10"
# !pip install --user "langchain==0.2.6"
# !pip install --user "langchain-ibm==0.1.8"
# !pip install --user "langchain-community==0.2.1"

"""
Importing required libraries
"""

# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_community.document_loaders import TextLoader
from langchain_ibm import WatsonxLLM

"""
Build LLM
"""

def llm_model(model_id):
    parameters = {
        GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output
        GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses
    }

    credentials = {
        "url": "https://us-south.ml.cloud.ibm.com"
    }

    project_id = "skills-network"

    model = ModelInference(
        model_id=model_id,
        params=parameters,
        credentials=credentials,
        project_id=project_id
    )

    llm = WatsonxLLM(watsonx_model = model)
    return llm

"""
Let's try to invoke an example query.
"""

llama_llm = llm_model('meta-llama/llama-3-3-70b-instruct')

llama_llm.invoke("How are you?")

"""
Load source document
"""

!wget "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/d_ahNwb1L2duIxBR6RD63Q/state-of-the-union.txt"

"""
Use `TextLoader` to load the text.
"""

loader = TextLoader("state-of-the-union.txt")

data = loader.load()

"""
Let's take a look at the document.
"""

content = data[0].page_content
content

"""
Limitation of retrieve directly from full document

Context length

LangChain prompt template
"""

template = """According to the document content here
            {content},
            answer this question
            {question}.
            Do not try to make up the answer.

            YOUR RESPONSE:
"""

prompt_template = PromptTemplate(template=template, input_variables=['content', 'question'])
prompt_template

"""
Use mixtral model
"""

mixtral_llm = llm_model('mistralai/mixtral-8x7b-instruct-v01')

"""
Then, create a query chain.
"""

query_chain = LLMChain(llm=mixtral_llm, prompt=prompt_template)

"""
Then, set the query and get the answer.
"""

query = "It is in which year of our nation?"
response = query_chain.invoke(input={'content': content, 'question': query})
print(response['text'])

"""
Use Llama 3 model
"""

query_chain = LLMChain(llm=llama_llm, prompt=prompt_template)
query_chain

"""
Then, use the query chain (the code is shown below) to invoke the LLM, which will answer the same query as before based on the entire document's content.
"""

query = "It is in which year of our nation?"
response = query_chain.invoke(input={'content': content, 'question': query})
print(response['text'])

"""
Take away

# Exercises

### Exercise 1 - Change to use another LLM

Try to use another LLM to see if error occurs. For example, try using `'ibm/granite-3-8b-instruct'`.
"""

granite_llm = llm_model('ibm/granite-3-8b-instruct')
query_chain = LLMChain(llm=granite_llm, prompt=prompt_template)
query = "It is in which year of our nation?"
response = query_chain.invoke(input={'content': content, 'question': query})
print(response['text'])

# Clearly we see that again it provides the correct answer.

granite_llm = llm_model('ibm/granite-3-8b-instruct')
query_chain = LLMChain(llm=granite_llm, prompt=prompt_template)
query = "It is in which year of our nation?"
response = query_chain.invoke(input={'content': content, 'question': query})
print(response['text'])


### Other Contributors

[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo),

Joseph has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.

Copyright Â© IBM Corporation. All rights reserved.
"""
